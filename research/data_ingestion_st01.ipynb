{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.NEFT50.Utils import Create_Directory,read_yaml_file\n",
    "from src.NEFT50.loggers import logger\n",
    "from src.NEFT50.Exception import CustomException\n",
    "from src.NEFT50.Constants import CONFIG_FILEPATH,PARAM_FILEPATH\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "#step3)update the entity file --->is nothing we r defining the class variable\n",
    "#which was used in yaml file and futhure taking rtn as function\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig():\n",
    "    #defining class variable along with dtypes\n",
    "    root_dir_path:Path\n",
    "    train_test_path: Path\n",
    "    raw_file_path:Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step4)update the configurationmanager file which was present in src/config/configuration.py\n",
    "#In this file we are reading yaml file ,create directory and also \n",
    "#assigning the value to the class variable and taking rtn as function\n",
    "\n",
    "class ConfigurationManager():\n",
    "    #initializing the instance variable \n",
    "    def __init__(self,config_filepath=CONFIG_FILEPATH,param_filepath=PARAM_FILEPATH):\n",
    "        #reading the yaml file\n",
    "        self.config = read_yaml_file(config_filepath) #rtn value as configdictatonary\n",
    "        self.param = read_yaml_file(param_filepath) #rtn value as configdictatonary\n",
    "        print(self.config)\n",
    "\n",
    "        #creating main directory in project structure\n",
    "        Create_Directory([self.config.artifacts_root]) #it will create artifact directory\n",
    "\n",
    "    #creating method to initialize value to dataingestion\n",
    "    def data_ingestion(self) ->DataIngestionConfig:\n",
    "        #initializing local variable\n",
    "        config = self.config.data_ingestion #here we r accessing dataingestion block from yaml file\n",
    "\n",
    "        #creating dataingestion root_dir_path\n",
    "        Create_Directory([config.root_dir_path])\n",
    "\n",
    "        #creating an object of DataIngestionConfig class and initialize class variable value to it \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir_path=config.root_dir_path,\n",
    "            train_test_path=config.train_test_path,\n",
    "            raw_file_path = config.raw_file_path\n",
    "         \n",
    "        )\n",
    "        return data_ingestion_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step5)update the components files!!! in this file \n",
    "class DataIngestion():\n",
    "    #constructor method initialize the class variable to object\n",
    "    def __init__(self,ingestionconfig:DataIngestionConfig):\n",
    "        self.ingestionconfig = ingestionconfig\n",
    "    \n",
    "    def train_test_data(self):\n",
    "        raw_data = self.ingestionconfig.raw_file_path\n",
    "\n",
    "        logger.info(f\"Loading the csv file {raw_data}\")\n",
    "\n",
    "        df_raw = pd.read_csv(raw_data)\n",
    "        \n",
    "        df_raw['date'] = pd.to_datetime(df_raw['date'])\n",
    "        \n",
    "        # Extract target column name\n",
    "        df_raw['target'] = df_raw['close'].shift(-1) #shift(-1) moves the close column up by 1 row\n",
    "        df_raw.dropna(inplace=True)\n",
    "\n",
    "        logger.info(f\"splitting the Raw dataset\")\n",
    "\n",
    "        train_df,test_df = train_test_split(df_raw,test_size=0.2,random_state=42)\n",
    "\n",
    "        #saving training and testing data\n",
    "        train_df.to_csv(\n",
    "            os.path.join(self.ingestionconfig.train_test_path, \"train.csv\"),\n",
    "            index=False,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        test_df.to_csv(\n",
    "            os.path.join(self.ingestionconfig.train_test_path, \"test.csv\"),\n",
    "            index=False,\n",
    "            encoding='utf-8'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\NEFT50_RegressionModel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-06 15:16:14,091]-INFO-19-Reading the YAML file config\\config.yaml\n",
      "{'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir_path': 'artifacts/data_ingestion', 'train_test_path': 'artifacts/data_ingestion/', 'raw_file_path': 'D:\\\\NEFT50_RegressionModel\\\\INDIA VIX_minute.csv'}, 'data_transformation': {'root_dir_path': 'artifacts/data_transformation', 'save_obj_dirpath': 'artifacts/data_transformation/preprocessor.pkl', 'csv_dir_path': 'artifacts/data_ingestion/'}}\n",
      "[2025-10-06 15:16:14,095]-INFO-23-YAML file read successfully: config\\config.yaml\n",
      "[2025-10-06 15:16:14,096]-INFO-19-Reading the YAML file param.yaml\n",
      "{'test_key': 'test_value'}\n",
      "[2025-10-06 15:16:14,098]-INFO-23-YAML file read successfully: param.yaml\n",
      "{'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir_path': 'artifacts/data_ingestion', 'train_test_path': 'artifacts/data_ingestion/', 'raw_file_path': 'D:\\\\NEFT50_RegressionModel\\\\INDIA VIX_minute.csv'}, 'data_transformation': {'root_dir_path': 'artifacts/data_transformation', 'save_obj_dirpath': 'artifacts/data_transformation/preprocessor.pkl', 'csv_dir_path': 'artifacts/data_ingestion/'}}\n",
      "[2025-10-06 15:16:14,100]-INFO-33-Creating Directory\n",
      "[2025-10-06 15:16:14,131]-INFO-37-Directory Created at artifacts\n",
      "[2025-10-06 15:16:14,132]-INFO-33-Creating Directory\n",
      "[2025-10-06 15:16:14,135]-INFO-37-Directory Created at artifacts/data_ingestion\n",
      "[2025-10-06 15:16:14,136]-INFO-10-Loading the csv file D:\\NEFT50_RegressionModel\\INDIA VIX_minute.csv\n",
      "[2025-10-06 15:16:15,404]-INFO-20-splitting the Raw dataset\n"
     ]
    }
   ],
   "source": [
    "#step6)update the training pipeline file\n",
    "try:\n",
    "    #creating an object of configurationmanager class\n",
    "    cm = ConfigurationManager()\n",
    "\n",
    "    data_ingestion_config = cm.data_ingestion()\n",
    "\n",
    "    #creating an object of DataIngestion component class\n",
    "    di = DataIngestion(ingestionconfig = data_ingestion_config)\n",
    "\n",
    "    di.train_test_data()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
