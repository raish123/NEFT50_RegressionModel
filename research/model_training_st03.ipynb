{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.NEFT50.Utils import Create_Directory,read_yaml_file,Save_object\n",
    "from src.NEFT50.loggers import logger\n",
    "from src.NEFT50.Exception import CustomException\n",
    "from src.NEFT50.Constants import CONFIG_FILEPATH,PARAM_FILEPATH\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3) update the entity file:- entity file is nothing but whatever parameter we have used in yaml file \n",
    "#we gonna defined them as a class variable\n",
    "@dataclass\n",
    "class ModelTrainingConfig():\n",
    "    #defining the class variable\n",
    "    root_dir_path:Path\n",
    "    save_best_model_dirpath:Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig():\n",
    "    #defining class variable along with dtypes\n",
    "    root_dir_path:Path\n",
    "    save_obj_dirpath: Path\n",
    "    csv_dir_path: Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the configurationmanager file in this file we read yaml file and create directories \n",
    "#and assigining values to DataTransformationConfig Class variable \n",
    "\n",
    "class ConfigurationManager():\n",
    "    #creating constructor to initialize the instance variable\n",
    "    def __init__(self,config_filepath = CONFIG_FILEPATH,param_filepath=PARAM_FILEPATH):\n",
    "        self.config = read_yaml_file(config_filepath) #rtn as configbox dictatonary\n",
    "        self.param =  read_yaml_file(param_filepath) #rtn as configbox dictionary\n",
    "    \n",
    "\n",
    "        #creating artifacts directory in the project structure\n",
    "        Create_Directory([self.config.artifacts_root]) #it will create artifacts directory\n",
    "\n",
    "    def get_data_transformation_config(self):\n",
    "        #creating local variable which was used inside this method\n",
    "        transform = self.config.data_transformation\n",
    "\n",
    "        #creating root directory in artifacts folder for datatransformation\n",
    "        Create_Directory([transform.root_dir_path]) #create artifacts/data_transformation folder\n",
    "\n",
    "        #creating an object &\n",
    "        #assigining the value to DataTransformationConfig class variable and taking rtn as function\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir_path=transform.root_dir_path,\n",
    "            save_obj_dirpath=transform.save_obj_dirpath,\n",
    "            csv_dir_path=transform.csv_dir_path,\n",
    "           \n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "    \n",
    "    \n",
    "    #another method we used to get model training config!!!\n",
    "    def get_model_training_config(self) ->ModelTrainingConfig:\n",
    "        #initializing the local variable which is used inside this method only\n",
    "        config = self.config.model_training\n",
    "      \n",
    "\n",
    "        #creating directory artifacts/model_training\n",
    "        Create_Directory([config.root_dir_path])\n",
    "\n",
    "        #creating an object of class variable and assigning value to parameter and taking rtn as fuctn\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir_path = config.root_dir_path,\n",
    "            save_best_model_dirpath=config.save_best_model_dirpath,\n",
    "     \n",
    "        )\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np,sklearn\n",
    "from sklearn.pipeline import Pipeline #this class we used to create pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer #to fill null value\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.base import BaseEstimator,TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step-5) updating the component file of Datatransformation and initializing the class variable as instance variance\n",
    "class DataTransformation():\n",
    "    def __init__(self,transformconfig:DataTransformationConfig):\n",
    "        self.transformconfig = transformconfig #this value we used in our datatransformation stage mei\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        #In this file we create preprocessor object which is futhure used to transformation\n",
    "        df = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "        \n",
    "        target_column_name = \"target\"\n",
    "\n",
    "        #selecting input and output variable\n",
    "        x = df[['open', 'high', 'low', 'close', 'volume']]\n",
    "        y = df[target_column_name]\n",
    "\n",
    "        #selecting object and numeric column from input variable\n",
    "        num_feature_lst = x.select_dtypes(exclude='object').columns.to_list()\n",
    "        cat_feature_lst = x.select_dtypes(include='object').columns.to_list()\n",
    "\n",
    "        logger.info(f\"Numeric column from input feature\\n%s\",num_feature_lst)\n",
    "        logger.info(f\"Categorical column from input feature\\n%s\",cat_feature_lst)\n",
    "\n",
    "        #creating numeric pipeline by using Pipeline class\n",
    "        numeric_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"median\")),#filling the numeric featuere will median\n",
    "            (\"scaling\",StandardScaler(with_mean=False))\n",
    "        ])\n",
    "        logger.info(f\"Numeric Pipeline feature\\n%s\",numeric_pipeline)\n",
    "\n",
    "        #creating categorical pipeline by using Pipeline class\n",
    "        categorical_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),#filling the categorical feature\n",
    "            (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "        logger.info(f\"Categorical Pipeline feature\\n%s\",categorical_pipeline)\n",
    "\n",
    "        #combining both pipelien using columntransformer class\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            (\"num_pipeline\",numeric_pipeline,num_feature_lst),\n",
    "            (\"cat_pipeline\",categorical_pipeline,cat_feature_lst)\n",
    "        ])\n",
    "\n",
    "        # #now saving the object into artifacts folder\n",
    "        # save_object(file=self.transformconfig.save_obj_dirpath,obj=preprocessor)\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        logger.info('Reading train and test Data Using Pandas Library')\n",
    "        train_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "        test_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"test.csv\"))\n",
    "        \n",
    "        \n",
    "        # Extract target column name\n",
    "        target_column_name = \"target\"\n",
    "\n",
    "        #selecting input and output variable from both train,test df object\n",
    "        train_input_feature = train_data.drop(target_column_name,axis=1)\n",
    "        train_output_feature = train_data[target_column_name]\n",
    "\n",
    "        test_input_feature = test_data.drop(target_column_name,axis=1)\n",
    "        test_output_feature = test_data[target_column_name]\n",
    "\n",
    "        #calling the preprocessor object\n",
    "        preprocessor_obj = self.get_data_transformation()\n",
    "\n",
    "        #now saving the object into artifacts folder\n",
    "        Save_object(filepath=Path(self.transformconfig.save_obj_dirpath),object=preprocessor_obj)\n",
    "\n",
    "        #applying this preprocessor object to input variable only for both train and test df object\n",
    "        input_feature_train_array  = preprocessor_obj.fit_transform(train_input_feature) #changes to 2d numpy array\n",
    "        input_feature_test_array  = preprocessor_obj.transform(test_input_feature)  #changes to 2d numpy array\n",
    "\n",
    "        logger.info('Combining  input feature train array with train_data_output_feature---->to get train_numpy_array')\n",
    "        train_numpy_array = np.c_[input_feature_train_array,np.array(train_output_feature)]\n",
    "        test_numpy_array = np.c_[input_feature_test_array,np.array(test_output_feature)]\n",
    "\n",
    "        return(\n",
    "            train_numpy_array,\n",
    "            test_numpy_array\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step5) update the component file: In this file we create an object for class varibale\n",
    "#and perform model training task accordingly\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class ModelTraining():\n",
    "    def __init__(self,modelconfig:ModelTrainingConfig):\n",
    "        self.modelconfig = modelconfig\n",
    "\n",
    "    #create kar raha hu method to perform training of model through grid search cv and get best param from it\n",
    "    def initiate_model_training(self,train_numpy_array,test_numpy_array):\n",
    "\n",
    "        #now selecting training and testing data from numpy array obj\n",
    "        x_train, y_train = train_numpy_array[:,:-1],train_numpy_array[:,-1]\n",
    "        x_test, y_test = test_numpy_array[:,:-1],test_numpy_array[:,-1]\n",
    "\n",
    "        # Initialize Linear Regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Train model\n",
    "        model.fit(x_train, y_train)\n",
    "        logger.info(\"Model training completed successfully.\")\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        # Evaluate model performance\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        logger.info(f\"Linear Regression Performance -> R2: {r2:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        # Save model only if performance is acceptable\n",
    "        if r2 > 0.50:\n",
    "            Save_object(\n",
    "                filepath=Path(self.modelconfig.save_best_model_dirpath),\n",
    "                object=model\n",
    "            )\n",
    "            logger.info(\"Model saved successfully as best model.\")\n",
    "        else:\n",
    "            logger.warning(\"Model R² score below threshold (0.50). Not saving the model.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\NEFT50_RegressionModel'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-06 17:34:12,483]-INFO-19-Reading the YAML file config\\config.yaml\n",
      "{'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir_path': 'artifacts/data_ingestion', 'train_test_path': 'artifacts/data_ingestion/', 'raw_file_path': 'D:\\\\NEFT50_RegressionModel\\\\INDIA VIX_minute.csv'}, 'data_transformation': {'root_dir_path': 'artifacts/data_transformation', 'save_obj_dirpath': 'artifacts/data_transformation/preprocessor.pkl', 'csv_dir_path': 'artifacts/data_ingestion/'}, 'model_training': {'root_dir_path': 'artifacts/model_training', 'save_best_model_dirpath': 'artifacts/model_training/models.pkl'}}\n",
      "[2025-10-06 17:34:12,486]-INFO-23-YAML file read successfully: config\\config.yaml\n",
      "[2025-10-06 17:34:12,487]-INFO-19-Reading the YAML file param.yaml\n",
      "{'test_key': 'test_value'}\n",
      "[2025-10-06 17:34:12,488]-INFO-23-YAML file read successfully: param.yaml\n",
      "[2025-10-06 17:34:12,489]-INFO-33-Creating Directory\n",
      "[2025-10-06 17:34:12,489]-INFO-33-Creating Directory\n",
      "[2025-10-06 17:34:12,490]-INFO-33-Creating Directory\n",
      "[2025-10-06 17:34:12,491]-INFO-37-Directory Created at artifacts/model_training\n",
      "[2025-10-06 17:34:13,016]-INFO-20-Numeric column from input feature\n",
      "['open', 'high', 'low', 'close', 'volume']\n",
      "[2025-10-06 17:34:13,017]-INFO-21-Categorical column from input feature\n",
      "[]\n",
      "[2025-10-06 17:34:13,021]-INFO-28-Numeric Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaling', StandardScaler(with_mean=False))])\n",
      "[2025-10-06 17:34:13,025]-INFO-35-Categorical Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
      "[2025-10-06 17:34:13,036]-INFO-49-Reading train and test Data Using Pandas Library\n",
      "[2025-10-06 17:34:14,183]-INFO-20-Numeric column from input feature\n",
      "['open', 'high', 'low', 'close', 'volume']\n",
      "[2025-10-06 17:34:14,183]-INFO-21-Categorical column from input feature\n",
      "[]\n",
      "[2025-10-06 17:34:14,184]-INFO-28-Numeric Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaling', StandardScaler(with_mean=False))])\n",
      "[2025-10-06 17:34:14,187]-INFO-35-Categorical Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
      "[2025-10-06 17:34:14,198]-INFO-43-Here in Utils we r Creating Save object Function will Save model and preprocessor files to artifacts Folder\n",
      "[2025-10-06 17:34:14,199]-INFO-45-Now Checking Filepath Exist or not\n",
      "[2025-10-06 17:34:14,201]-INFO-50-Object Save Into Artfacts Folder\n",
      "[2025-10-06 17:34:14,630]-INFO-74-Combining  input feature train array with train_data_output_feature---->to get train_numpy_array\n",
      "[2025-10-06 17:34:14,729]-INFO-22-Model training completed successfully.\n",
      "[2025-10-06 17:34:14,738]-INFO-32-Linear Regression Performance -> R2: 0.9999, MSE: 0.0053, RMSE: 0.0730\n",
      "[2025-10-06 17:34:14,739]-INFO-43-Here in Utils we r Creating Save object Function will Save model and preprocessor files to artifacts Folder\n",
      "[2025-10-06 17:34:14,739]-INFO-45-Now Checking Filepath Exist or not\n",
      "[2025-10-06 17:34:14,743]-INFO-50-Object Save Into Artfacts Folder\n",
      "[2025-10-06 17:34:14,744]-INFO-40-Model saved successfully as best model.\n"
     ]
    }
   ],
   "source": [
    "#step6) update the pipeline file\n",
    "try:\n",
    "    cm = ConfigurationManager() #object of configuration manager class\n",
    "    \n",
    "    #creating an object of dataclasses\n",
    "    data_transform_config = cm.get_data_transformation_config()\n",
    "    model_config = cm.get_model_training_config()\n",
    "\n",
    "    #creating an object of datatransformation class\n",
    "    dt = DataTransformation(transformconfig=data_transform_config)\n",
    "\n",
    "    dt.get_data_transformation()\n",
    "\n",
    "    train_array,test_array = dt.initiate_data_transformation()\n",
    "\n",
    "    #creating an object of ModelTraining clas\n",
    "    mt = ModelTraining(model_config)\n",
    "    mt.initiate_model_training(\n",
    "        train_numpy_array=train_array,\n",
    "        test_numpy_array=test_array,\n",
    "    )\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
