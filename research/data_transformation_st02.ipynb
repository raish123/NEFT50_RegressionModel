{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.NEFT50.Utils import Create_Directory,read_yaml_file,Save_object\n",
    "from src.NEFT50.loggers import logger\n",
    "from src.NEFT50.Exception import CustomException\n",
    "from src.NEFT50.Constants import CONFIG_FILEPATH,PARAM_FILEPATH\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3)update the entity file --->is nothing we r defining the class variable\n",
    "#which was used in yaml file and futhure taking rtn as function\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig():\n",
    "    #defining class variable along with dtypes\n",
    "    root_dir_path:Path\n",
    "    save_obj_dirpath: Path\n",
    "    csv_dir_path: Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the configurationmanager file in this file we read yaml file and create directories \n",
    "#and assigining values to DataTransformationConfig Class variable \n",
    "\n",
    "class ConfigurationManager():\n",
    "    #creating constructor to initialize the instance variable\n",
    "    def __init__(self,config_filepath = CONFIG_FILEPATH,param_filepath=PARAM_FILEPATH):\n",
    "        self.config = read_yaml_file(config_filepath) #rtn as configbox dictatonary\n",
    "        self.param =  read_yaml_file(param_filepath) #rtn as configbox dictionary\n",
    "    \n",
    "\n",
    "        #creating artifacts directory in the project structure\n",
    "        Create_Directory([self.config.artifacts_root]) #it will create artifacts directory\n",
    "\n",
    "    def get_data_transformation_config(self):\n",
    "        #creating local variable which was used inside this method\n",
    "        transform = self.config.data_transformation\n",
    "\n",
    "        #creating root directory in artifacts folder for datatransformation\n",
    "        Create_Directory([transform.root_dir_path]) #create artifacts/data_transformation folder\n",
    "\n",
    "        #creating an object &\n",
    "        #assigining the value to DataTransformationConfig class variable and taking rtn as function\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir_path=transform.root_dir_path,\n",
    "            save_obj_dirpath=transform.save_obj_dirpath,\n",
    "            csv_dir_path=transform.csv_dir_path,\n",
    "           \n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np,sklearn\n",
    "from sklearn.pipeline import Pipeline #this class we used to create pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer #to fill null value\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.base import BaseEstimator,TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step-5) updating the component file of Datatransformation and initializing the class variable as instance variance\n",
    "class DataTransformation():\n",
    "    def __init__(self,transformconfig:DataTransformationConfig):\n",
    "        self.transformconfig = transformconfig #this value we used in our datatransformation stage mei\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        #In this file we create preprocessor object which is futhure used to transformation\n",
    "        df = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "        \n",
    "        target_column_name = \"target\"\n",
    "\n",
    "        #selecting input and output variable\n",
    "        x = df[['open', 'high', 'low', 'close', 'volume']]\n",
    "        y = df[target_column_name]\n",
    "\n",
    "        #selecting object and numeric column from input variable\n",
    "        num_feature_lst = x.select_dtypes(exclude='object').columns.to_list()\n",
    "        cat_feature_lst = x.select_dtypes(include='object').columns.to_list()\n",
    "\n",
    "        logger.info(f\"Numeric column from input feature\\n%s\",num_feature_lst)\n",
    "        logger.info(f\"Categorical column from input feature\\n%s\",cat_feature_lst)\n",
    "\n",
    "        #creating numeric pipeline by using Pipeline class\n",
    "        numeric_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"median\")),#filling the numeric featuere will median\n",
    "            (\"scaling\",StandardScaler(with_mean=False))\n",
    "        ])\n",
    "        logger.info(f\"Numeric Pipeline feature\\n%s\",numeric_pipeline)\n",
    "\n",
    "        #creating categorical pipeline by using Pipeline class\n",
    "        categorical_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),#filling the categorical feature\n",
    "            (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "        logger.info(f\"Categorical Pipeline feature\\n%s\",categorical_pipeline)\n",
    "\n",
    "        #combining both pipelien using columntransformer class\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            (\"num_pipeline\",numeric_pipeline,num_feature_lst),\n",
    "            (\"cat_pipeline\",categorical_pipeline,cat_feature_lst)\n",
    "        ])\n",
    "\n",
    "        # #now saving the object into artifacts folder\n",
    "        # save_object(file=self.transformconfig.save_obj_dirpath,obj=preprocessor)\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        logger.info('Reading train and test Data Using Pandas Library')\n",
    "        train_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"train.csv\"))\n",
    "        test_data = pd.read_csv(os.path.join(self.transformconfig.csv_dir_path,\"test.csv\"))\n",
    "        \n",
    "        \n",
    "        # Extract target column name\n",
    "        target_column_name = \"target\"\n",
    "\n",
    "        #selecting input and output variable from both train,test df object\n",
    "        train_input_feature = train_data.drop(target_column_name,axis=1)\n",
    "        train_output_feature = train_data[target_column_name]\n",
    "\n",
    "        test_input_feature = test_data.drop(target_column_name,axis=1)\n",
    "        test_output_feature = test_data[target_column_name]\n",
    "\n",
    "        #calling the preprocessor object\n",
    "        preprocessor_obj = self.get_data_transformation()\n",
    "\n",
    "        #now saving the object into artifacts folder\n",
    "        Save_object(filepath=Path(self.transformconfig.save_obj_dirpath),object=preprocessor_obj)\n",
    "\n",
    "        #applying this preprocessor object to input variable only for both train and test df object\n",
    "        input_feature_train_array  = preprocessor_obj.fit_transform(train_input_feature) #changes to 2d numpy array\n",
    "        input_feature_test_array  = preprocessor_obj.transform(test_input_feature)  #changes to 2d numpy array\n",
    "\n",
    "        logger.info('Combining  input feature train array with train_data_output_feature---->to get train_numpy_array')\n",
    "        train_numpy_array = np.c_[input_feature_train_array,np.array(train_output_feature)]\n",
    "        test_numpy_array = np.c_[input_feature_test_array,np.array(test_output_feature)]\n",
    "\n",
    "        return(\n",
    "            train_numpy_array,\n",
    "            test_numpy_array\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\NEFT50_RegressionModel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-06 15:16:31,694]-INFO-19-Reading the YAML file config\\config.yaml\n",
      "{'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir_path': 'artifacts/data_ingestion', 'train_test_path': 'artifacts/data_ingestion/', 'raw_file_path': 'D:\\\\NEFT50_RegressionModel\\\\INDIA VIX_minute.csv'}, 'data_transformation': {'root_dir_path': 'artifacts/data_transformation', 'save_obj_dirpath': 'artifacts/data_transformation/preprocessor.pkl', 'csv_dir_path': 'artifacts/data_ingestion/'}}\n",
      "[2025-10-06 15:16:31,699]-INFO-23-YAML file read successfully: config\\config.yaml\n",
      "[2025-10-06 15:16:31,700]-INFO-19-Reading the YAML file param.yaml\n",
      "{'test_key': 'test_value'}\n",
      "[2025-10-06 15:16:31,702]-INFO-23-YAML file read successfully: param.yaml\n",
      "[2025-10-06 15:16:31,703]-INFO-33-Creating Directory\n",
      "[2025-10-06 15:16:31,705]-INFO-33-Creating Directory\n",
      "[2025-10-06 15:16:31,706]-INFO-37-Directory Created at artifacts/data_transformation\n",
      "[2025-10-06 15:16:32,543]-INFO-20-Numeric column from input feature\n",
      "['open', 'high', 'low', 'close', 'volume']\n",
      "[2025-10-06 15:16:32,544]-INFO-21-Categorical column from input feature\n",
      "[]\n",
      "[2025-10-06 15:16:32,546]-INFO-28-Numeric Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaling', StandardScaler(with_mean=False))])\n",
      "[2025-10-06 15:16:32,582]-INFO-35-Categorical Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
      "[2025-10-06 15:16:32,604]-INFO-49-Reading train and test Data Using Pandas Library\n",
      "[2025-10-06 15:16:34,509]-INFO-20-Numeric column from input feature\n",
      "['open', 'high', 'low', 'close', 'volume']\n",
      "[2025-10-06 15:16:34,511]-INFO-21-Categorical column from input feature\n",
      "[]\n",
      "[2025-10-06 15:16:34,512]-INFO-28-Numeric Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaling', StandardScaler(with_mean=False))])\n",
      "[2025-10-06 15:16:34,515]-INFO-35-Categorical Pipeline feature\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
      "[2025-10-06 15:16:34,538]-INFO-43-Here in Utils we r Creating Save object Function will Save model and preprocessor files to artifacts Folder\n",
      "[2025-10-06 15:16:34,540]-INFO-45-Now Checking Filepath Exist or not\n",
      "[2025-10-06 15:16:34,543]-INFO-50-Object Save Into Artfacts Folder\n",
      "[2025-10-06 15:16:35,189]-INFO-74-Combining  input feature train array with train_data_output_feature---->to get train_numpy_array\n"
     ]
    }
   ],
   "source": [
    "#step6)updating the training pipeline\n",
    "try:\n",
    "    #creating an object of configuration manager\n",
    "    cm = ConfigurationManager()\n",
    "    data_transform_config = cm.get_data_transformation_config()\n",
    "\n",
    "    #creating an object of datatransformation class\n",
    "    dt = DataTransformation(transformconfig=data_transform_config)\n",
    "\n",
    "    dt.get_data_transformation()\n",
    "\n",
    "    train_array,test_array = dt.initiate_data_transformation()\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.04551842,  4.04329282,  4.04759583,  4.04547482,  0.        ,\n",
       "        24.98      ],\n",
       "       [ 1.85197002,  1.84947101,  1.85116317,  1.84871238,  0.        ,\n",
       "        11.43      ],\n",
       "       [ 2.71643854,  2.7143897 ,  2.69731657,  2.71479043,  0.        ,\n",
       "        16.62      ],\n",
       "       ...,\n",
       "       [ 2.79576243,  2.79360655,  2.79457558,  2.79411346,  0.        ,\n",
       "        17.29      ],\n",
       "       [ 3.14543509,  3.14280737,  3.146329  ,  3.14702003,  0.        ,\n",
       "        19.39      ],\n",
       "       [ 2.73262709,  2.72893974,  2.72649427,  2.72450345,  0.        ,\n",
       "        16.91      ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
